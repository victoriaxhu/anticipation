{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victoriaxhu/anticipation/blob/main/beam_search_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs"
      ],
      "metadata": {
        "id": "ZMZJmdUFvt_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fsdE4TbkvBED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf462fa-5ce7-44f7-88fc-1d786454d8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fluid-soundfont-gm libevdev2 libfluidsynth3 libgudev-1.0-0 libinput-bin\n",
            "  libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a libqt5dbus5\n",
            "  libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "Suggested packages:\n",
            "  fluid-soundfont-gs qt5-image-formats-plugins qtwayland5 jackd\n",
            "The following NEW packages will be installed:\n",
            "  fluid-soundfont-gm fluidsynth libevdev2 libfluidsynth3 libgudev-1.0-0\n",
            "  libinput-bin libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a\n",
            "  libqt5dbus5 libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "0 upgraded, 32 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 148 MB of archives.\n",
            "After this operation, 207 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluid-soundfont-gm all 3.1-5.3 [130 MB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 timgm6mb-soundfont all 1.3-5 [5,427 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluidsynth amd64 2.2.5-1 [27.4 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qsynth amd64 0.9.6-1 [305 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\n",
            "Fetched 148 MB in 5s (29.5 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libqt5core5a:amd64.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libevdev2:amd64.\n",
            "Preparing to unpack .../01-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
            "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Selecting previously unselected package libmtdev1:amd64.\n",
            "Preparing to unpack .../02-libmtdev1_1.1.6-1build4_amd64.deb ...\n",
            "Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../03-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libwacom-common.\n",
            "Preparing to unpack .../04-libwacom-common_2.2.0-1_all.deb ...\n",
            "Unpacking libwacom-common (2.2.0-1) ...\n",
            "Selecting previously unselected package libwacom9:amd64.\n",
            "Preparing to unpack .../05-libwacom9_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom9:amd64 (2.2.0-1) ...\n",
            "Selecting previously unselected package libinput-bin.\n",
            "Preparing to unpack .../06-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libinput10:amd64.\n",
            "Preparing to unpack .../07-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libmd4c0:amd64.\n",
            "Preparing to unpack .../08-libmd4c0_0.4.8-1_amd64.deb ...\n",
            "Unpacking libmd4c0:amd64 (0.4.8-1) ...\n",
            "Selecting previously unselected package libqt5dbus5:amd64.\n",
            "Preparing to unpack .../09-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5network5:amd64.\n",
            "Preparing to unpack .../10-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../11-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../12-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../13-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../14-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../15-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../16-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../17-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../18-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../19-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libqt5gui5:amd64.\n",
            "Preparing to unpack .../20-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5widgets5:amd64.\n",
            "Preparing to unpack .../21-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5svg5:amd64.\n",
            "Preparing to unpack .../22-libqt5svg5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "Preparing to unpack .../23-fluid-soundfont-gm_3.1-5.3_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.3) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../24-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package timgm6mb-soundfont.\n",
            "Preparing to unpack .../25-timgm6mb-soundfont_1.3-5_all.deb ...\n",
            "Unpacking timgm6mb-soundfont (1.3-5) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../26-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package fluidsynth.\n",
            "Preparing to unpack .../27-fluidsynth_2.2.5-1_amd64.deb ...\n",
            "Unpacking fluidsynth (2.2.5-1) ...\n",
            "Selecting previously unselected package libwacom-bin.\n",
            "Preparing to unpack .../28-libwacom-bin_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom-bin (2.2.0-1) ...\n",
            "Selecting previously unselected package qsynth.\n",
            "Preparing to unpack .../29-qsynth_0.9.6-1_amd64.deb ...\n",
            "Unpacking qsynth (0.9.6-1) ...\n",
            "Selecting previously unselected package qt5-gtk-platformtheme:amd64.\n",
            "Preparing to unpack .../30-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package qttranslations5-l10n.\n",
            "Preparing to unpack .../31-qttranslations5-l10n_5.15.3-1_all.deb ...\n",
            "Unpacking qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmd4c0:amd64 (0.4.8-1) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.3) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "Setting up timgm6mb-soundfont (1.3-5) ...\n",
            "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up libwacom-common (2.2.0-1) ...\n",
            "Setting up libwacom9:amd64 (2.2.0-1) ...\n",
            "Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Setting up fluidsynth (2.2.5-1) ...\n",
            "Created symlink /etc/systemd/user/default.target.wants/fluidsynth.service → /usr/lib/systemd/user/fluidsynth.service.\n",
            "Setting up libwacom-bin (2.2.0-1) ...\n",
            "Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Setting up qsynth (0.9.6-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install fluidsynth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jthickstun/anticipation.git\n",
        "!pip install ./anticipation\n",
        "!pip install -r anticipation/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwGwz9OqwX0N",
        "outputId": "e503d6e0-677b-4872-db58-777f12fcfd44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'anticipation'...\n",
            "remote: Enumerating objects: 1526, done.\u001b[K\n",
            "remote: Counting objects: 100% (351/351), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 1526 (delta 286), reused 275 (delta 236), pack-reused 1175 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1526/1526), 56.24 MiB | 21.69 MiB/s, done.\n",
            "Resolving deltas: 100% (1009/1009), done.\n",
            "Processing ./anticipation\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: anticipation\n",
            "  Building wheel for anticipation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for anticipation: filename=anticipation-1.0-py3-none-any.whl size=18682 sha256=40a95e49bac808b047de43f2231b5ddc1f19cf951ffeb39ad5a5c35686644544\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pitcvei4/wheels/00/47/a1/fce9dedfd7d5c624e471dc01096a22fd7c945799cf58510c11\n",
            "Successfully built anticipation\n",
            "Installing collected packages: anticipation\n",
            "Successfully installed anticipation-1.0\n",
            "Collecting matplotlib==3.7.1 (from -r anticipation/requirements.txt (line 1))\n",
            "  Downloading matplotlib-3.7.1.tar.gz (38.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting midi2audio==0.1.1 (from -r anticipation/requirements.txt (line 2))\n",
            "  Downloading midi2audio-0.1.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting mido==1.2.10 (from -r anticipation/requirements.txt (line 3))\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.12/dist-packages (from -r anticipation/requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from -r anticipation/requirements.txt (line 5)) (2.8.0+cu126)\n",
            "Collecting transformers==4.29.2 (from -r anticipation/requirements.txt (line 6))\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl.metadata (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.65.0 (from -r anticipation/requirements.txt (line 7))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (2.32.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2->-r anticipation/requirements.txt (line 6))\n",
            "  Downloading tokenizers-0.13.3.tar.gz (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.9/314.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.1->-r anticipation/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.1->-r anticipation/requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.29.2->-r anticipation/requirements.txt (line 6)) (2025.10.5)\n",
            "Downloading midi2audio-0.1.1-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: matplotlib, tokenizers\n",
            "  Building wheel for matplotlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-3.7.1-cp312-cp312-linux_x86_64.whl size=11092810 sha256=d83e9dfd5e364fefb5bf0b6eb97f0e3f75ce7fd94f88daba7a99903589a0a8b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/06/fa/3453aac11411fac092c1bdfe52815f2f6969a42700d977e62f\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully built matplotlib\n",
            "Failed to build tokenizers\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install \"midi2audio==0.1.1\"\n",
        "!pip install \"mido==1.2.10\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QGi84UEwYi-",
        "outputId": "044ae722-bdc6-4da6-ce71-8eefc7a285ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.5)\n",
            "Collecting midi2audio==0.1.1\n",
            "  Using cached midi2audio-0.1.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Using cached midi2audio-0.1.1-py2.py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: midi2audio\n",
            "Successfully installed midi2audio-0.1.1\n",
            "Collecting mido==1.2.10\n",
            "  Using cached mido-1.2.10-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Using cached mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "Installing collected packages: mido\n",
            "Successfully installed mido-1.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the runtime environment"
      ],
      "metadata": {
        "id": "9uFcdhHNwjMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,time\n",
        "\n",
        "import midi2audio\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "from anticipation import ops\n",
        "from anticipation.sample import generate\n",
        "from anticipation.tokenize import extract_instruments\n",
        "from anticipation.convert import events_to_midi,midi_to_events\n",
        "from anticipation.visuals import visualize\n",
        "from anticipation.config import *\n",
        "from anticipation.vocab import *"
      ],
      "metadata": {
        "id": "ZUxt5IPUwaZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SMALL_MODEL = 'stanford-crfm/music-small-800k'     # faster inference, worse sample quality\n",
        "MEDIUM_MODEL = 'stanford-crfm/music-medium-800k'   # slower inference, better sample quality\n",
        "LARGE_MODEL = 'stanford-crfm/music-large-800k'     # slowest inference, best sample quality\n",
        "\n",
        "# load an anticipatory music transformer\n",
        "model = AutoModelForCausalLM.from_pretrained(SMALL_MODEL).cuda()\n",
        "\n",
        "# a MIDI synthesizer\n",
        "fs = midi2audio.FluidSynth('/usr/share/sounds/sf2/FluidR3_GM.sf2')\n",
        "\n",
        "# the MIDI synthesis script\n",
        "def synthesize(fs, tokens):\n",
        "    mid = events_to_midi(tokens)\n",
        "    mid.save('tmp.mid')\n",
        "    fs.midi_to_audio('tmp.mid', 'tmp.wav')\n",
        "    return 'tmp.wav'"
      ],
      "metadata": {
        "id": "vjN4AZiLwlK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from anticipation import ops\n",
        "from anticipation.config import *\n",
        "from anticipation.vocab import *"
      ],
      "metadata": {
        "id": "kS5AlDAWwqtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom functions"
      ],
      "metadata": {
        "id": "qmpa4Gq3wsnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_logits(logits, idx):\n",
        "    logits[CONTROL_OFFSET:SPECIAL_OFFSET] = -float('inf') # don't generate controls\n",
        "    logits[SPECIAL_OFFSET:] = -float('inf')               # don't generate special tokens\n",
        "\n",
        "    # don't generate stuff in the wrong time slot\n",
        "    if idx % 3 == 0:\n",
        "        logits[DUR_OFFSET:DUR_OFFSET+MAX_DUR] = -float('inf')\n",
        "        logits[NOTE_OFFSET:NOTE_OFFSET+MAX_NOTE] = -float('inf')\n",
        "    elif idx % 3 == 1:\n",
        "        logits[TIME_OFFSET:TIME_OFFSET+MAX_TIME] = -float('inf')\n",
        "        logits[NOTE_OFFSET:NOTE_OFFSET+MAX_NOTE] = -float('inf')\n",
        "    elif idx % 3 == 2: #expecting a note token\n",
        "        logits[TIME_OFFSET:TIME_OFFSET+MAX_TIME] = -float('inf')\n",
        "        logits[DUR_OFFSET:DUR_OFFSET+MAX_DUR] = -float('inf')\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def nucleus(logits, top_p):\n",
        "    # from HF implementation\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = -float(\"inf\")\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def future_logits(logits, curtime):\n",
        "    \"\"\" don't sample events in the past \"\"\"\n",
        "    if curtime > 0:\n",
        "        logits[TIME_OFFSET:TIME_OFFSET+curtime] = -float('inf')\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def instr_logits_part1(logits, full_history, instruments):\n",
        "    \"\"\" don't sample more than 16 instruments \"\"\"\n",
        "    instrs = ops.get_instruments(full_history)\n",
        "    print(\"instruments full history\", instrs)\n",
        "\n",
        "    if instruments is not None:\n",
        "    #ONLY ALLOW SPECIFIED INSTRUMENTS, BE CAREFUL -- which instruments are present in full_history?\n",
        "        #print(\"ONLY ALLOW SPECIFIED INSTRUMENTS\")\n",
        "        for instr_id in range(128):\n",
        "            if instr_id not in instruments:\n",
        "                #print(\"block instrument\", instr_id)\n",
        "                logits[NOTE_OFFSET+instr_id*MAX_PITCH:NOTE_OFFSET+(instr_id+1)*MAX_PITCH] = -float('inf')\n",
        "            else:\n",
        "                print(\"allowed instruemtn\", instr_id)\n",
        "\n",
        "    if len(instrs) < 15: # 16 - 1 to account for the reserved drum track\n",
        "        return logits\n",
        "\n",
        "    for instr in range(MAX_INSTR):\n",
        "        if instr not in instrs: #only use instruments in instrs, which i guess means from full_history it should be instruments used in the past\n",
        "            logits[NOTE_OFFSET+instr*MAX_PITCH:NOTE_OFFSET+(instr+1)*MAX_PITCH] = -float('inf')\n",
        "\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def add_token_part1(model, z, tokens, top_p, current_time, instruments, debug=False):\n",
        "    assert len(tokens) % 3 == 0\n",
        "\n",
        "    history = tokens.copy()\n",
        "    lookback = max(len(tokens) - 1017, 0)\n",
        "    history = history[lookback:] # Markov window\n",
        "    offset = ops.min_time(history, seconds=False)\n",
        "    history[::3] = [tok - offset for tok in history[::3]] # relativize time in the history buffer\n",
        "\n",
        "    new_token = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(3):\n",
        "            input_tokens = torch.tensor(z + history + new_token).unsqueeze(0).to(model.device)\n",
        "            logits = model(input_tokens).logits[0,-1]\n",
        "\n",
        "            idx = input_tokens.shape[1]-1\n",
        "            logits = safe_logits(logits, idx)\n",
        "            if i == 0:\n",
        "                logits = future_logits(logits, current_time - offset)\n",
        "            elif i == 2:\n",
        "                logits = instr_logits_part1(logits, tokens, instruments) #PASS DOWN THE RESTRICTION HERE\n",
        "            logits = nucleus(logits, top_p)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            token = torch.multinomial(probs, 1)\n",
        "            new_token.append(int(token))\n",
        "\n",
        "    new_token[0] += offset # revert to full sequence timing\n",
        "    if debug:\n",
        "        print(f'  OFFSET = {offset}, LEN = {len(history)}, TIME = {tokens[::3][-5:]}')\n",
        "    print(\"new token: \", new_token[0], new_token[1], new_token[2])\n",
        "\n",
        "    return new_token\n"
      ],
      "metadata": {
        "id": "qgP60slpxTdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from math import inf\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class Beam:\n",
        "    tokens: List[int]                  #full token history (context + generated)\n",
        "    score: float = 0.0                 #sum of log-probs for generated tokens\n",
        "    current_time: float = 0.0          #last generated absolute TIME (after offset)\n",
        "    control_tokens: List[int] = field(default_factory=list)  #put controls per-beam rather than global in case times don't align\n",
        "    anticip_time: float = inf          #onset of next anticipatory triple (ATIME - ATIME_OFFSET)\n",
        "    gen_len: int = 0"
      ],
      "metadata": {
        "id": "jhNyKP5DxcWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_logits(logits, idx):\n",
        "    logits[CONTROL_OFFSET:SPECIAL_OFFSET] = -float('inf') # don't generate controls\n",
        "    logits[SPECIAL_OFFSET:] = -float('inf')               # don't generate special tokens\n",
        "\n",
        "    # don't generate stuff in the wrong time slot\n",
        "    if idx % 3 == 0:\n",
        "        logits[DUR_OFFSET:DUR_OFFSET+MAX_DUR] = -float('inf')\n",
        "        logits[NOTE_OFFSET:NOTE_OFFSET+MAX_NOTE] = -float('inf')\n",
        "    elif idx % 3 == 1:\n",
        "        logits[TIME_OFFSET:TIME_OFFSET+MAX_TIME] = -float('inf')\n",
        "        logits[NOTE_OFFSET:NOTE_OFFSET+MAX_NOTE] = -float('inf')\n",
        "    elif idx % 3 == 2: #expecting a note token\n",
        "        logits[TIME_OFFSET:TIME_OFFSET+MAX_TIME] = -float('inf')\n",
        "        logits[DUR_OFFSET:DUR_OFFSET+MAX_DUR] = -float('inf')\n",
        "\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "szXDxXnbxgw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nucleus(logits, top_p):\n",
        "    # from HF implementation\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = -float(\"inf\")\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "j0bdsgBcxird"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def future_logits(logits, curtime):\n",
        "    \"\"\" don't sample events in the past \"\"\"\n",
        "    if curtime > 0:\n",
        "        logits[TIME_OFFSET:TIME_OFFSET+curtime+1] = -float('inf')\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "mL3aWrChxlNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def instr_logits_part1(logits, full_history, instruments):\n",
        "    \"\"\" don't sample more than 16 instruments \"\"\"\n",
        "    instrs = ops.get_instruments(full_history)\n",
        "    #print(\"instruments full history\", instrs)\n",
        "\n",
        "    if instruments is not None:\n",
        "    #ONLY ALLOW SPECIFIED INSTRUMENTS, BE CAREFUL -- which instruments are present in full_history?\n",
        "        #print(\"ONLY ALLOW SPECIFIED INSTRUMENTS\")\n",
        "        for instr_id in range(128):\n",
        "            if instr_id not in instruments:\n",
        "                #print(\"block instrument\", instr_id)\n",
        "                logits[NOTE_OFFSET+instr_id*MAX_PITCH:NOTE_OFFSET+(instr_id+1)*MAX_PITCH] = -float('inf')\n",
        "            #else:\n",
        "                #print(\"allowed instruemtn\", instr_id)\n",
        "\n",
        "    if len(instrs) < 15: # 16 - 1 to account for the reserved drum track\n",
        "        return logits\n",
        "\n",
        "    for instr in range(MAX_INSTR):\n",
        "        if instr not in instrs: #only use instruments in instrs, which i guess means from full_history it should be instruments used in the past\n",
        "            logits[NOTE_OFFSET+instr*MAX_PITCH:NOTE_OFFSET+(instr+1)*MAX_PITCH] = -float('inf')\n",
        "\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "fsXKxoYKxnnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topk_triples(model, z, tokens, current_time, instruments, debug=False, K_time=4, K_dur=2, K_note=2, K_total=8, top_p=None):\n",
        "    assert len(tokens) % 3 == 0\n",
        "\n",
        "    device=model.device\n",
        "\n",
        "    history = tokens.copy()\n",
        "    lookback = max(len(tokens) - 1017, 0)\n",
        "    history = history[lookback:] # Markov window\n",
        "    offset = ops.min_time(history, seconds=False)\n",
        "    history[::3] = [tok - offset for tok in history[::3]] # relativize time in the history buffer\n",
        "\n",
        "    def apply_masks(logits, phase_idx, inp_len, tokens): #uhhh compared to original code inp_len is basically input_tokens.shape[1]\n",
        "        logits = safe_logits(logits, inp_len - 1)\n",
        "        if phase_idx == 0:\n",
        "            logits = future_logits(logits, current_time - offset)\n",
        "        elif phase_idx == 2:\n",
        "            logits = instr_logits_part1(logits, tokens, instruments)\n",
        "        if top_p is not None:\n",
        "            logits = nucleus(logits, top_p)\n",
        "        return logits\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #TIME TOKEN: generate K_time possibilities\n",
        "        inp0 = torch.tensor(z + history, device=device).unsqueeze(0)\n",
        "        logits_t = model(inp0).logits[0, -1] #(1, L, V) -> just shape V\n",
        "        logits_t = apply_masks(logits_t, phase_idx=0, inp_len=inp0.shape[1], tokens=tokens)\n",
        "        logp_t = torch.log_softmax(logits_t, dim=-1)\n",
        "        t_vals, t_ids = torch.topk(logp_t, K_time)\n",
        "        t_ids = t_ids.tolist(); t_vals = t_vals.tolist()\n",
        "\n",
        "        #DURATION TOKEN (batch over K_time)\n",
        "        #build batch prefixes: z + history + [t_i]\n",
        "        batch_time_inputs = [z + history + [t] for t in t_ids]\n",
        "        inp1 = torch.nn.utils.rnn.pad_sequence(\n",
        "            [torch.tensor(x, device=device) for x in batch_time_inputs],\n",
        "            batch_first=True\n",
        "        )\n",
        "        logits_d_all = model(inp1).logits[:, -1, :] #[K_time, L, V] -> [K_time, V]\n",
        "        logp_d_all = []\n",
        "        d_ids_all  = []\n",
        "        for row, base_len in zip(logits_d_all, [len(x) for x in batch_time_inputs]):\n",
        "            row = apply_masks(row, phase_idx=1, inp_len=base_len, tokens=tokens)\n",
        "            #note tokens is the \"old\" full history but it's ok here, the only thing apply_masks\n",
        "            #passes it into is instr_logit which cares about instrument history we're chilling\n",
        "            lp = torch.log_softmax(row, dim=-1)\n",
        "            d_vals, d_ids = torch.topk(lp, K_dur) #take top K_dur options\n",
        "            logp_d_all.append(d_vals)\n",
        "            d_ids_all.append(d_ids)\n",
        "        #shapes end up being lists of K_time tensors, each [K_dur]\n",
        "\n",
        "        #NOTE TOKEN (batch over K_time *K_dur)\n",
        "        td_pairs = []\n",
        "        td_logps = []\n",
        "        td_inputs = []\n",
        "        for i in range(len(t_ids)): #K_time outer loop\n",
        "            for j in range(K_dur): #K_dur inner loop\n",
        "                time_token_id = t_ids[i] #token id\n",
        "                lp_time = t_vals[i] #log prob for that token\n",
        "                dur_token_id = d_ids_all[i][j].item()\n",
        "                lp_dur = logp_d_all[i][j].item()\n",
        "                td_pairs.append((time_token_id, dur_token_id, lp_time, lp_dur))\n",
        "                td_inputs.append(z + history + [time_token_id, dur_token_id])\n",
        "\n",
        "        inp2 = torch.nn.utils.rnn.pad_sequence( #batched processing (K_time*K_dur) batches\n",
        "            [torch.tensor(x, device=device) for x in td_inputs],\n",
        "            batch_first=True\n",
        "        )\n",
        "        logits_n_all = model(inp2).logits[:, -1, :] #(K_time*K_dur, L, V) -> (K_time*K_dur, V)\n",
        "\n",
        "        candidates = []  # (triple_ids, joint_logp)\n",
        "        idx = 0\n",
        "        for i in range(len(t_ids)): #K_time\n",
        "            for j in range(K_dur): #K_dur\n",
        "                row = logits_n_all[idx]\n",
        "                idx += 1 #counts up to K_time*K_dur\n",
        "                base_len = len(td_inputs[i*K_dur + j])\n",
        "                row = apply_masks(row, phase_idx=2, inp_len=base_len, tokens=tokens)\n",
        "                lp = torch.log_softmax(row, dim=-1)\n",
        "                note_vals, note_ids = torch.topk(lp, K_note) #pick top K_note options\n",
        "                time_token_id, dur_token_id, lp_time, lp_dur = td_pairs[i*K_dur + j]\n",
        "                for k in range(K_note):\n",
        "                    note_token_id = note_ids[k].item()\n",
        "                    lp_note = note_vals[k].item()\n",
        "                    joint = lp_time + lp_dur + lp_note\n",
        "                    candidates.append(([time_token_id, dur_token_id, note_token_id], joint))\n",
        "\n",
        "        def dedup(candidates):\n",
        "            unique = {}\n",
        "            for note_choice, logprob in candidates:\n",
        "                key = tuple(note_choice)\n",
        "                if key not in unique or logprob > unique[key]:\n",
        "                    unique[key] = logprob #tuplify the array of 3\n",
        "            return [(list(key), unique[key]) for key in unique.keys()]\n",
        "\n",
        "        candidates = dedup(candidates) #remove duplicates hopefully this helps\n",
        "\n",
        "        #candidates has list of triples ([time token, dur token, note token], prob)\n",
        "        joint_logps = torch.tensor([logprob for _, logprob in candidates], device=device)\n",
        "        #gumbel top k sampling, basically the idea is you add random noise before you take the top k\n",
        "        u = torch.rand_like(joint_logps)\n",
        "        g = -torch.log(-torch.log(u))              # Gumbel(0,1)\n",
        "        tau = 1.0                                  # temperature: 1.0–1.5 = good range\n",
        "        scores = joint_logps / tau + g             # random jittered scores\n",
        "\n",
        "        #choose K_total without replacement (highest noised scores)\n",
        "        top = torch.topk(scores, K_total)\n",
        "        best = [candidates[i] for i in top.indices.tolist()]\n",
        "\n",
        "        #ALTERNATIVELY, DETERMINISTIC TOP TOTAL_K -> tried this and it led to beam collapse\n",
        "        #candidates.sort(key=lambda x: x[1], reverse=True) #highest to lowest by joint prob\n",
        "        #best = candidates[:K_total]\n",
        "\n",
        "        triples = torch.tensor([ids for ids,_ in best], device=device, dtype=torch.long)  # [K_total, 3]\n",
        "        logps  = torch.tensor([lp  for _,lp in best], device=device, dtype=torch.float)   # [K_total]\n",
        "\n",
        "    #if TIME in history was relativized by `offset`, undo for output:\n",
        "    triples[:, 0] = triples[:, 0] + offset\n",
        "    print(\"triple of tokens option from single beam\", triples)\n",
        "\n",
        "    return triples, logps #FORMAT IS TENSOR OF SHAPES [K_total, 3] and [K_total]\n"
      ],
      "metadata": {
        "id": "Ioe0Vtg0xrGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_beams(model, start_time, end_time, inputs=None, controls=None, top_p=None,\n",
        "                   debug=False, delta=DELTA*TIME_RESOLUTION, instruments=None,\n",
        "                   num_beams=10, K_total=10, K_time=5, K_dur=2, K_note=3):\n",
        "    if inputs is None:\n",
        "        inputs = []\n",
        "\n",
        "    if controls is None:\n",
        "        controls = []\n",
        "\n",
        "    start_time = int(TIME_RESOLUTION*start_time)\n",
        "    end_time = int(TIME_RESOLUTION*end_time)\n",
        "\n",
        "    # prompt is events up to start_time\n",
        "    prompt = ops.pad(ops.clip(inputs, 0, start_time, clip_duration=False, seconds=False), start_time)\n",
        "\n",
        "    # treat events beyond start_time as controls\n",
        "    future = ops.clip(inputs, start_time+1, ops.max_time(inputs, seconds=False), clip_duration=False, seconds=False)\n",
        "    if debug:\n",
        "        print('Future')\n",
        "        ops.print_tokens(future)\n",
        "\n",
        "    # clip controls that preceed the sequence\n",
        "    controls = ops.clip(controls, DELTA, ops.max_time(controls, seconds=False), clip_duration=False, seconds=False)\n",
        "\n",
        "    if debug:\n",
        "        print('Controls')\n",
        "        ops.print_tokens(controls)\n",
        "\n",
        "    z = [ANTICIPATE] if len(controls) > 0 or len(future) > 0 else [AUTOREGRESS]\n",
        "    if debug:\n",
        "        print('AR Mode' if z[0] == AUTOREGRESS else 'AAR Mode')\n",
        "\n",
        "    # interleave the controls with the events\n",
        "    tokens, controls = ops.anticipate(prompt, ops.sort(controls + [CONTROL_OFFSET+token for token in future]))\n",
        "\n",
        "    if debug:\n",
        "        print('Prompt')\n",
        "        ops.print_tokens(tokens)\n",
        "\n",
        "    current_time = ops.max_time(prompt, seconds=False)\n",
        "\n",
        "    if debug:\n",
        "        print('Current time:', current_time)\n",
        "\n",
        "    #ok now we make a list of beams each initializing tokens with the controls\n",
        "    beams = []\n",
        "    for _ in range(num_beams):\n",
        "        beams.append(Beam(\n",
        "          tokens=tokens.copy(),\n",
        "          control_tokens=controls.copy(),\n",
        "          anticip_time=(controls[0] - ATIME_OFFSET if controls else math.inf),\n",
        "          score=0.0,\n",
        "          current_time=current_time,\n",
        "          gen_len = 0\n",
        "        ))\n",
        "\n",
        "    #with tqdm(range(end_time-start_time)) as progress:\n",
        "    not_done = True\n",
        "\n",
        "    while not_done:\n",
        "\n",
        "        candidates = []\n",
        "        unique_beams = {}\n",
        "        not_done = False\n",
        "\n",
        "        for idx, beam in enumerate(beams): #add a token triplet in each one which hasn't finished\n",
        "\n",
        "            if beam.current_time >= end_time:\n",
        "                candidates.append(beam)\n",
        "                continue\n",
        "\n",
        "            not_done = True #if at least one beam gets token added, then not done\n",
        "            #last pass not_done will be False if every beam is done\n",
        "            print(\"beam\", idx, \"has current time\", beam.current_time, \"tokens\", beam.tokens)\n",
        "\n",
        "            #directly mutate control_tokens, anticip_time\n",
        "            while beam.current_time >= beam.anticip_time - delta:\n",
        "\n",
        "                if not beam.control_tokens:\n",
        "                    break\n",
        "\n",
        "                atime, adur, anote = beam.control_tokens[:3]\n",
        "                beam.tokens.extend([atime, adur, anote])\n",
        "                beam.control_tokens = beam.control_tokens[3:]\n",
        "\n",
        "                if debug:\n",
        "                    note = anote - ANOTE_OFFSET\n",
        "                    instr = note//2**7\n",
        "                    print('A', atime - ATIME_OFFSET, adur - ADUR_OFFSET, instr, note - (2**7)*instr)\n",
        "\n",
        "                if len(beam.control_tokens) > 0:\n",
        "                    beam.anticip_time = beam.control_tokens[0] - ATIME_OFFSET\n",
        "                else:\n",
        "                    beam.anticip_time = math.inf\n",
        "\n",
        "\n",
        "            new_triples, logps = topk_triples(model, z, beam.tokens, max(start_time,beam.current_time), instruments=instruments, K_total=K_total,\n",
        "                                              debug=debug, K_time=K_time, K_dur=K_dur, K_note=K_note, top_p=None)\n",
        "            #also has default parameters (debug=False, K_time=4, K_dur=2, K_note=2, K_total=8, top_p=None)\n",
        "            #shapes [K_total, 3] and [K_total]\n",
        "\n",
        "            for row, logp in zip(new_triples, logps):\n",
        "\n",
        "                  new_time = row[0].item() - TIME_OFFSET\n",
        "                  if new_time < beam.current_time:\n",
        "                      continue\n",
        "\n",
        "                  if new_time < end_time:\n",
        "                      possible_beam = Beam(\n",
        "                          tokens=beam.tokens.copy() + [token.item() for token in row],\n",
        "                          control_tokens=beam.control_tokens[:],\n",
        "                          anticip_time = beam.anticip_time,\n",
        "                          score=beam.score + logp.item(),\n",
        "                          current_time=new_time, #the new time, don't actually mutate new_triples\n",
        "                          gen_len = beam.gen_len+3\n",
        "                      )\n",
        "                  else: #DON'T ACTUALLY APPEND THAT TRIPLE, though anticipation has been mutated\n",
        "                      possible_beam = Beam(\n",
        "                          tokens=beam.tokens.copy(),\n",
        "                          control_tokens=beam.control_tokens[:],\n",
        "                          anticip_time = beam.anticip_time,\n",
        "                          score=beam.score,\n",
        "                          current_time=new_time, #terminal beam candidate, kill it from growing to prevent inf loop\n",
        "                          gen_len=beam.gen_len\n",
        "                      )\n",
        "\n",
        "                  if tuple(possible_beam.tokens) not in unique_beams: #DEDUP\n",
        "                      unique_beams[tuple(possible_beam.tokens)] = True\n",
        "                      candidates.append(possible_beam)\n",
        "\n",
        "            if debug:\n",
        "                print(\"print data about the new triples generated?\")\n",
        "                #new_note = new_token[2] - NOTE_OFFSET\n",
        "                #new_instr = new_note//2**7\n",
        "                #new_pitch = new_note - (2**7)*new_instr\n",
        "                #print('C', new_time, new_token[1] - DUR_OFFSET, new_instr, new_pitch)\n",
        "\n",
        "        def rank(b, alpha=0.5, gamma=0.01, empty_penalty=1e6, start_tick=0):\n",
        "            #normalize for length\n",
        "            base = b.score / (max(1, b.gen_len) ** alpha)\n",
        "\n",
        "            #favor beams that advance forward in time\n",
        "            #be careful to scale gamma to your tick units / seconds, start_tick is start_time in ticks\n",
        "            prog = gamma * max(0, b.current_time - start_tick)\n",
        "\n",
        "            #add penalty for empty generations so that we don't just end up with prompt and nothing else\n",
        "            if b.gen_len == 0:\n",
        "                return base + prog - empty_penalty\n",
        "\n",
        "            return base + prog\n",
        "\n",
        "        candidates.sort(key=lambda b: rank(b), reverse=True) #highest to lowest by score, in-place sort\n",
        "        beams = candidates[:num_beams]\n",
        "        # for beam in beams:\n",
        "        #     print(\"current time\", beam.current_time, \"tokens\", beam.tokens)\n",
        "\n",
        "\n",
        "    #NOW CHOOSE FINAl OUTPUT OFF OF BEAMS LIST\n",
        "    if beams:\n",
        "        best_tokens = beams[0].tokens\n",
        "    else:\n",
        "        best_tokens = tokens\n",
        "\n",
        "    print(\"best_tokens\", best_tokens)\n",
        "    events, _ = ops.split(best_tokens)\n",
        "    return ops.sort(ops.unpad(events) + future)"
      ],
      "metadata": {
        "id": "l45q_RqBxtQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing\n"
      ],
      "metadata": {
        "id": "z_8AuApgxuGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tokens= generate_beams(model, start_time=0, end_time=5, top_p=.98, instruments={0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 40, 41}, debug=False,\n",
        "                              num_beams=10, K_total=10, K_time=5, K_dur=2, K_note=5) #K_total is branch factor\n",
        "Audio(synthesize(fs, sample_tokens))"
      ],
      "metadata": {
        "id": "RoWtQY7OxvEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}